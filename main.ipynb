{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da15d8db",
   "metadata": {},
   "source": [
    "# Bin Packing Problem (BPP)\n",
    "\n",
    "Online Bin Packing Problem with bins containing 1D resource.\n",
    "\n",
    "state/action/reward formulation are inspired from:  https://epub.jku.at/obvulihs/download/pdf/6996324?originalFilename=true\n",
    "\n",
    "Short description (P22,P23):\n",
    "* **BPP specification:** 50 items of size 2 and 50 of size 3. Bins have a capacity of 9.\n",
    "* **Observable state:** [B_0, B_1, ... B_n-1, I]. B_i is the number of bins of filling level i. I is the new incoming item size.\n",
    "* **Action space:**  Select a bin according its filling level. The action 0 means \"we open a new bin\".\n",
    "* **Reward at a given timestep:** 0 if an already existing bin is used, otherwise the negative incremental waste (reward=BIN_SIZE - I). Penalty of -1000 is given if we enter in one of those situation: bin overflow or when the action try to select an innexisting bin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111f42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_factory import rl_agent_factory # Factory to build RL agents\n",
    "from rl_factory import default_hyperparam_factory # Factor to make easier hyperparameter setting\n",
    "from EnvBinPacking import EnvBinPacking # <- Definition of the BPP. Global variables contains bins size and items distribution, number of items...\n",
    "from AgentRL import AgentRLLIB\n",
    "from AgentHeuristic import AgentBestFit\n",
    "\n",
    "# Environment pointer\n",
    "env_config = {\"action_type\": \"discrete\"} # PPO implementation fits {\"continuous\" or \"discrete\"}. DQN \"discrete\". DDPG \"continuous\".\n",
    "env_class=EnvBinPacking # ptr on the environment class (not an OOP object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b33a4d",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782d792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 16:48:12,307\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='<class 'EnvBinPacking.EnvBinPacking'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'EnvBinPacking.EnvBinPacking'>').build()` instead. This will raise an error in the future!\n",
      "2023-02-01 16:48:14,569\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27498)\u001b[0m 2023-02-01 16:48:20,880\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-02-01 16:48:23,993\tINFO trainable.py:172 -- Trainable.setup took 11.652 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-02-01 16:48:23,995\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# build the RL agent\n",
    "rl_name = \"PPO\" \n",
    "hyperparameters = default_hyperparam_factory(rl_name)\n",
    "\n",
    "# update value similar to the publication\n",
    "hyperparameters[\"lr\"]=1e-3\n",
    "hyperparameters[\"deep\"]=2\n",
    "hyperparameters[\"wide\"]=16\n",
    "hyperparameters[\"train_batch_size\"]=64\n",
    "hyperparameters[\"sgd_minibatch_size\"]=64\n",
    "hyperparameters[\"lambda\"]=0.99\n",
    "hyperparameters[\"grad_clip\"]=0.3\n",
    "\n",
    "# Build the Trainer (contains RL object and Environment simulator object)\n",
    "rllib_trainer = rl_agent_factory(rl_name, hyperparameters, env_class, env_config=env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b975ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: -1278.0\n",
      "Info: -1048.0\n",
      "Info: -1012.0\n",
      "Info: -1024.0\n",
      "Info: -1398.0\n",
      "Info: -611.0\n",
      "Info: -637.0\n",
      "Info: -516.0\n",
      "Info: -630.0\n",
      "Info: -606.0\n"
     ]
    }
   ],
   "source": [
    "agent=AgentRLLIB(rllib_trainer, env_class, env_config) \n",
    "for ep in range(10):\n",
    "    for it in range(10):\n",
    "        agent.train()\n",
    "\n",
    "    score=agent.evaluate()\n",
    "    print(\"Info:\", score['cumulated_rewards'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ea319",
   "metadata": {},
   "source": [
    "## Heuristics (Best Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7f44790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: -193.0\n"
     ]
    }
   ],
   "source": [
    "agent = AgentBestFit(None, env_class, env_config)\n",
    "score = agent.evaluate()\n",
    "print(\"Info:\", score['cumulated_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2bf55a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cumulated_rewards': -193.0, 'last_reward': -7.0, 'last_state': array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0., 13.,  0.], dtype=float32), 'last_action': 0}\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2c596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
