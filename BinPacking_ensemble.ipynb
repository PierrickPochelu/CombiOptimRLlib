{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da15d8db",
   "metadata": {},
   "source": [
    "# Bin Packing Problem (BPP)\n",
    "\n",
    "Comparing RL agent vs Ensemble of agents for solving BPP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111f42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl_factory import rl_agent_factory, default_hyperparam_factory  # Factory to build RL agents name:stirng->model:rllib\n",
    "from BinPacking.EnvBinPacking import EnvBinPacking, inv_prepro_state, inv_prepro_reward\n",
    "from AgentRL import AgentRLLIB\n",
    "from EnsembleAgentRL import EnsembleAgentRL\n",
    "\n",
    "\n",
    "# Environment\n",
    "env_config = {\"action_type\": \"discrete\"} # PPO implementation fits {\"continuous\" or \"discrete\"}. DQN \"discrete\". DDPG \"continuous\".\n",
    "env_class=EnvBinPacking # ptr on the environment class (not an OOP object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b33a4d",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782d792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 15:35:45,842\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='<class 'BinPacking.EnvBinPacking.EnvBinPacking'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'BinPacking.EnvBinPacking.EnvBinPacking'>').build()` instead. This will raise an error in the future!\n",
      "2023-02-04 15:35:49,061\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11909)\u001b[0m 2023-02-04 15:35:55,817\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-02-04 15:35:59,431\tINFO trainable.py:172 -- Trainable.setup took 13.224 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-02-04 15:35:59,433\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# build the RL agent\n",
    "rl_name = \"PPO\" \n",
    "hyperparameters = default_hyperparam_factory(rl_name)\n",
    "\n",
    "# update value similar to the publication\n",
    "hyperparameters[\"lr\"]=1e-4 # <---Sensisitive between slow-smooth and fast-chaotic . Original paper uses 3e-4.\n",
    "hyperparameters[\"deep\"]=2\n",
    "hyperparameters[\"wide\"]=16\n",
    "hyperparameters[\"train_batch_size\"]=64\n",
    "hyperparameters[\"sgd_minibatch_size\"]=64\n",
    "hyperparameters[\"lambda\"]=0.99\n",
    "hyperparameters[\"grad_clip\"]=0.3\n",
    "hyperparameters[\"num_rollout_workers\"]=4\n",
    "\n",
    "# Build the Trainer (contains RL object and Environment simulator object)\n",
    "rllib_trainer = rl_agent_factory(rl_name, hyperparameters, env_class, env_config=env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac14a3",
   "metadata": {},
   "source": [
    "## Agents training\n",
    "\n",
    "We train `nb_agents` and store their best version during the training step based on regular intermediate evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eccf210-db76-4779-90ca-0921ce265aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_agents = 4\n",
    "episodes = 1000\n",
    "eval_every = 100\n",
    "eval_times = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b975ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : 0\n",
      "Training : 1\n",
      "Training : 2\n",
      "Training : 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AGENTS_INFO = dict()  # {id -> {\"path\"->path:str, \"score\"->score:int, \"history\"->hist:List[int]}  }\n",
    "\n",
    "for a in range(nb_agents):\n",
    "    agent = AgentRLLIB(rllib_trainer, env_class, env_config,\n",
    "                       inv_prepro_state=inv_prepro_state, inv_prepro_reward=inv_prepro_reward)\n",
    "\n",
    "    agent_info = {\"folder\": \"/tmp/ensemble/agent\" + str(a) + \"/\",\n",
    "                  \"checkpoint_path\": \"undefined\",\n",
    "                  \"score\": -np.inf,\n",
    "                  \"history\": []}\n",
    "\n",
    "    print(f\"Training : {a}\")\n",
    "    for i in range(episodes):\n",
    "        agent.train()\n",
    "\n",
    "        if i % eval_every == 0:\n",
    "            score = np.round(np.mean([agent.evaluate()[\"cumulated_rewards\"] for i in range(eval_times)]), 2)\n",
    "            agent_info[\"history\"].append(score)\n",
    "\n",
    "            # better score found\n",
    "            if score >= agent_info[\"score\"]:\n",
    "                agent_info[\"score\"] = score\n",
    "                agent_info[\"checkpoint_path\"] = agent.save(agent_info[\"folder\"])\n",
    "\n",
    "    AGENTS_INFO[a] = agent_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ea319",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90484ea6",
   "metadata": {},
   "source": [
    "Ensemble construction for boosting cumulated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bf55a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 15:36:25,634\tINFO trainable.py:790 -- Restored on 192.168.1.100 from checkpoint: /tmp/ensemble/agent0/checkpoint_002401\n",
      "2023-02-04 15:36:25,635\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 2401, '_timesteps_total': None, '_time_total': 459.4775776863098, '_episodes_total': 3790}\n",
      "2023-02-04 15:36:25,681\tINFO trainable.py:790 -- Restored on 192.168.1.100 from checkpoint: /tmp/ensemble/agent1/checkpoint_015101\n",
      "2023-02-04 15:36:25,682\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 15101, '_timesteps_total': None, '_time_total': 2812.099847793579, '_episodes_total': 13058}\n",
      "2023-02-04 15:36:25,727\tINFO trainable.py:790 -- Restored on 192.168.1.100 from checkpoint: /tmp/ensemble/agent2/checkpoint_025501\n",
      "2023-02-04 15:36:25,729\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 25501, '_timesteps_total': None, '_time_total': 4752.843599319458, '_episodes_total': 20088}\n",
      "2023-02-04 15:36:25,771\tINFO trainable.py:790 -- Restored on 192.168.1.100 from checkpoint: /tmp/ensemble/agent3/checkpoint_036301\n",
      "2023-02-04 15:36:25,772\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 36301, '_timesteps_total': None, '_time_total': 6886.798044681549, '_episodes_total': 27864}\n"
     ]
    }
   ],
   "source": [
    "ensemble = []\n",
    "for agent_info in AGENTS_INFO.values():\n",
    "    # build an empty RL agent\n",
    "    agent = AgentRLLIB(rllib_trainer, env_class, env_config,\n",
    "                       inv_prepro_state=inv_prepro_state, inv_prepro_reward=inv_prepro_reward)\n",
    "\n",
    "    # restore weights\n",
    "    agent.restore(agent_info[\"checkpoint_path\"])\n",
    "\n",
    "    # save it\n",
    "    ensemble.append(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93257114",
   "metadata": {},
   "source": [
    "##Â Test agents and ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d5aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test agent 0 score: -515.33\n",
      "Test agent 1 score: -492.0\n",
      "Test agent 2 score: -510.6\n",
      "Test agent 3 score: -533.9\n",
      "Test ensemble score: -487.03\n"
     ]
    }
   ],
   "source": [
    "for a, agent in enumerate(ensemble):\n",
    "    score = np.round(np.mean([agent.evaluate()[\"cumulated_rewards\"] for i in range(eval_times)]), 2)\n",
    "    print(f\"Test agent {a} score: {score}\")\n",
    "\n",
    "ensemble_agent = EnsembleAgentRL(ensemble)\n",
    "score = np.round(np.mean([ensemble_agent.evaluate()[\"cumulated_rewards\"] for i in range(eval_times)]), 2)\n",
    "print(\"Test ensemble score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5099f53-a4ff-4356-babb-c18a97f6f818",
   "metadata": {},
   "source": [
    "Conclusion: Ensemble of RL agents is an easy way to boost cumulated rewards but multiply the computing time at both training and inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600eb48d-15dd-4e07-9941-d1c7c52ea6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
